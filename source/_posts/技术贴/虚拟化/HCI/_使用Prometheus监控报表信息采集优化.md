---
title: 使用Prometheus监控报表信息采集优化.md
tags:
  - null
categories:
  - technical
  - Prometheus
toc: true
declare: true
date: 2022-01-11 16:10:20
---

# 一、开发目的/原因

两个问题：

未来HCI平台需要支持大量的虚拟机（48个物理节点2000台虚拟机），每个虚拟机都独立保存自己的报表数据，获取报表数据要跨主机通信。虚拟机过多这样的操作性能就极具下降
HCI中的很多服务程序自带独立采集所需数据，缺乏一个统一的规划，导致数据大量的重复采集
对现有的数据采集、存储、消费等相关流程进行梳理，并重新设计，统一规划，希望最终达成这样的目标：

全部的数据使用统一的框架采集，并集中存储。
HCI其他模块不再独自采集数据，需要数据时，从报表和状态中心获取。

# 二、旧版架构与问题

每个节点部署一个vtp-datareportd（报表数据抓取服务），以及一个vtp-datareport-server（报表服务主程序，接收数据，写入RRD，响应外部查询请求），每个节点各自保存本节点的报表数据到本地RRD文件。

报表数据 > image2022-1-10_19-15-43.png

<!-- more -->

这个方案存在几个问题：

数据分布存储。
查询时需从相应节点获取数据，涉及跨主机通信，性能相对较差。
不便于导出数据、聚合分析。
若虚拟机运行位置切换，只能查询到该虚拟机在新节点上运行产生的趋势数据。（也就是旧报表数据没有移植保存）
      2. 重复数据采集。

除了报表服务之外，还有很多服务（比如，告警，DRS等等）独自采集数据，独自维护状态，存在重复的数据采集行为，浪费系统资源。

# 三、新架构

新版的报表服务，全部的数据使用统一的框架采集，并集中存储，有效避免上述问题。其工作原理如下：

计算节点：
每个节点部署一个Exporter实例，用于采集本节点的指标数据，包括虚拟机，物理机，虚拟机网络等等；  （本节点的指标数据）

同时部署一个Pushgateway实例，用于接收本节点自定义脚本推送的指标数据，供Prometheus抓取。 （本节点其他指标数据（用自定义脚本实现））

主控节点：

主控节点部署一个Prometheus实例，负责定时从所有节点的Exporter和Pushgateway拉取数据，并将其中的状态数据发送给状态中心statuscenterd，趋势数据则写入内置的TSDB。

Prometheus自身只保留最近1小时数据（热数据，位于内存中），更长时间的历史数据使用RRD进行归档。rrd-adapter每隔1分钟向Prometheus发送http请求获取数据，经过解析转换之后归档到RRD文件。

statuscenterd负责响应和分发外部请求，最近1小时数据从Prometheus获取，更长时间的历史数据则从RRD文件获取。

主控每隔1小时向两个备选节点增量同步RRD所在的loop文件（使用rsync）。

报表数据 > image2022-1-10_19-18-37.png

# 四、应用场景

报表数据 > image2022-1-10_19-32-5.png

报表数据 > image2022-1-10_19-33-21.png报表数据 > image2022-1-10_19-33-33.png

# 五、Prometheus架构的改进

1.问题
长时间历史数据存储问题。

Prometheus本身自带基于本地存储的TSDB，但不支持Downsampling（降采样，即将多个细粒度的数据点通过均值等方式，合并成一个粗粒度的数据点），无法存储长时间的历史数据。

高可用问题。

Prometheus是一个单机方案，本身不提供高可用能力。

解决：

2. 降采样问题
存储长时间的历史数据，通常的做法都是降采样（Downsampling）。即通过降低数据精度，来达到存储更长时间历史数据的目的。比如，12个5秒精度的点，通过计算均值，合并成1个1分钟精度的点。

替换一个支持降采样的时序数据库：

InfluxDB：简单不依赖第三方环境，操作函数丰富，但是资源消耗较大

OpenTSDB：依赖于HBase，存储量大且快，但是架构也更复杂

RRD：使用固定大小的空间来存储数据，并有一个指针指向最新的数据的位置，圆心指向每个存储点，没有结束点，当一个圆都存储满了之后就将就将最开始的数据覆盖；非常简单，但是要自研高可用

3. 高可用解决
主要解决两个方面的高可用：

Prometheus的高可用
RRD的高可用

方案：

基本HA：

报表数据 > image2022-1-10_20-13-57.png

基本的HA模式只能确保Promthues服务的可用性问题，但是不解决Prometheus Server之间的数据一致性问题以及持久化问题(数据丢失后无法恢复)，也无法进行动态的扩展。

基本HA + 远程存储：

报表数据 > image2022-1-10_20-15-10.png

在解决了Promthues服务可用性的基础上，同时确保了数据的持久化，当Promthues Server发生宕机或者数据丢失的情况下，可以快速的恢复。 同时Promthues Server可以很好的进行迁移。

基本HA + 远程存储 + 联邦集群:

报表数据 > image2022-1-10_20-16-23.png



场景一：单数据中心 + 大量的采集任务
将不同类型的采集任务划分到不同的Promthues子服务中，从而实现功能分区。例如一个Promthues Server负责采集基础设施相关的监控指标，另外一个Prometheus Server负责采集应用监控指标。再有上层Prometheus Server实现对数据的汇聚。


场景二：多数据中心

当Promthues Server无法直接与数据中心中的Exporter进行通讯时，在每一个数据中部署一个单独的Promthues Server负责当前数据中心的采集任务是一个不错的方式。

这样可以避免用户进行大量的网络配置，只需要确保主Promthues Server实例能够与当前数据中心的Prometheus Server通讯即可。

每个控制节点部署一个Prometheus实例，场景比较适合，可以列为备选方案之一，带远端存储的方案，对部署有要求，不太适合HCI的使用场景，所以不考虑。

方案一：

报表数据 > image2022-1-10_20-21-58.png

每个控制节点部署一个Prometheus实例，仲裁节点除外。
当Exporter收到主控制节点Prometheus实例的pull请求时，开始采集本节点当前指标数据，并在内存中缓存数据；当Exporter收到其他控制节点Prometheus实例的pull请求时，直接返回内存中缓存的数据，避免重复采集。
当主控离线时，其他控制节点报表服务仍然可正常工作，基本没有切换开销。
总的来说，方案1就是堆计算资源，来换取服务的高可用。

方案二：主控部署Prometheus实例+rsync同步（选择的方案）

报表数据 > image2022-1-10_20-25-40.png



其他控制节点使用rsync定时增量同步RRD文件。
当主控离线时，从其他正常的控制节点中挑选一个节点，启动Prometheus和报表服务（statuscenterd）实例。
只同步一个控制节点，切换主控有问题。有可能会切到没有数据的控制节点上。做成可配置的，默认只同步到一个控制节点。数据量不大，可以同步2份

rrd-adapter适配器
Prometheus和RRD之间对接需要一个适配器来做格式转换工作：

适配器定时向Prometheus的实例发送http请求，获取所需数据。再将接收到的数据经过格式转换之后，存入rrd数据库。

4.数据采集方式
Prometheus将所有采集到的数据都按照时序数据来存储，并且内存缓存了最近一段时间的指标数据。

对于有些数据，我们只关心其当前值，不关心其趋势，这类状态数据如果也按时序数据来存储，则会存在内存和磁盘资源的浪费。所以，对于只需要当前值的状态数据，需要更改其存储方式。

我们又希望状态数据和趋势数据的采集，可以共用一套采集框架。而Prometheus启动时，因为要加载数据到内存，会比较耗时，这期间无法采集状态数据，对于状态中心服务来说，这一点不可接受。

方案1：引入一个中间层Prometheus
报表数据 > image2022-1-10_20-36-4.png

引入一个中间层Prometheus，定时采集数据，并缓存到内存，同时它还接收从Pushgateway推送过来的数据，并将状态数据发送到状态中心服务statuscenterd。

优点：

独立组件，不依赖Prometheus，可独立运行。当实例挂掉时，可以立即被拉起，启动耗时较短。
不涉及Prometheus源码改动，开源组件与业务解耦，更便于维护。
支持完全的推送机制，可以从计算节点底层脚本推送Pushgateway，Pushgateway再推送到控制节点Prometheus，Prometheus再推送到其他接收方。


缺点：

虽不涉及源码改动，但基本相当于在外部实现一个scrape组件。诸如配置解析，定时采集，服务发现等等，都需要中间层Prometheus来处理。存在冗余。
数据时效性相对较差，中间层Prometheus定时采集之后，再被Prometheus定时采集，时效性较差。
方案2：改造Prometheus内部scrape组件（选择）
报表数据 > image2022-1-10_20-41




