---
title: 项目说明
tags:
  - null
categories:
  - technical
  - null
toc: true
declare: true
date: 2021-12-26 16:34:34
---

[TOC]

# 一、解决方法1: golang并发版本

github地址：https://github.com/xwjahahahaha/filterFile.git

## 1.1 API使用说明

### 1. FilterFilesByBlackList

* 作用：通过一个或多个黑名单文件过滤掉一个或多个目标文件中在黑名单文件同样出现的字符串行（每个目标文件都会被所有黑名单文件过滤），并对应生成结果文件。注意，结果文件不会保留目标文件的空行。
* 入参：
  * `blackListFilePath`: 所有黑名单文件路径
  * `targetFilePath`: 所有目标文件路径
  * `resultFilePath`: 结果文件保存的目标文件夹
  * `order`: 是否允许结果文件字符串乱序
* 返回值：成功为nil，错误为error

```go
func FilterFilesByBlackList(blackListFilePath, targetFilePath []string, resultFilePath string, order bool) error
```

### 2. FilterFilesByRegexp

* 作用：通过一个正则表达式文件过滤掉一个或多个目标文件中匹配该正则表达式文件所有正则字符串的所有字符串行，并对应生成结果文件。注意，结果文件不会保留目标文件中的空行
* 入参：
  * `regexpFile`: 正则表达式文件路径
  * `targetFilePath`: 所有目标文件路径
  * `resultFilePath`: 结果文件保存的目标文件夹
  * `order`: 是否允许结果文件字符串乱序
* 返回值：成功为nil，错误为error

```go
func FilterFilesByRegexp(regexpFile string, targetFilePath []string, resultFilePath string, order bool) error
```

## 1.2 目录结构

```shell
./
├── go_resolution					# go解决方案
│   ├── blocklist.txt			# 提供的示例黑名单
│   ├── filter						# 过滤器包文件	
│   ├── pprof							# 测试生成的prof文件
│   ├── result.txt				# 使用filter过滤的结果
│   └── target.txt				# 提供的示例目标文件
```

## 1.3 实现思路

### 1. 总体框架

![image-20211226191547066](http://xwjpics.gumptlu.work/qinniu_uPic/image-20211226191547066.png)

说明：

1. 多个黑名单文件、目标文件多个协程并发读取
2. 对于一个大文件首先按一个块缓冲区大小分给多个协程处理（块最后不满一行向下多读直到一行结束`\n`）
3. 每个协程在块中按行扫描字符串，使用Sync.Map去重保存生成任务数据结构放入`BTChannel`保存
4. 如果支持结果乱序，则一个文件的多个块之间不需等待（`wg.wait`）同步进行，对于每个协程负责的块按`\n`切割后，再按固定负责字符串个数再次分为多个子协程处理
5. 一个协程不断从`BTChannel`中读取任务，合并其中的`Map`生成最终的`Map`
6. 目标文件每个协程匹配是否存在的复杂度为`O(1)`，黑名单文件与目标文件都是遍历一遍的复杂度

### 2. 代码设计

**抽象数据结构：**

* `Filter`接口： 每一种过滤方式的抽象

  ```go
  type Filter interface {
  	// 初始化一个过滤器
  	InitFilter(targetFiles []string, resultPath string, filteFiles ...string) error
  	// 创建所有过滤任务
  	CreateAllTasks() error
  	// 所有文件过滤所有任务
  	FilterAllFilesByTask(filteFunc func(*sync.Map, string) bool, overChan chan bool) error
  }
  ```

  * `BlackFilter` 实例：根据黑名单过滤方式的过滤器

    ```go
    type BlackFilter struct {
    	BlackListFiles []string
    	TargetFiles    []string
    	ResultPath     string
    	Order          bool // 是否有序扫描
    	BTChannel      chan *BlackTask
    	ErrChannel     chan error
    }
    ```

  * `RegexpFilter`实例：根据正则表达式文件方式的过滤器

    ```go
    type RegexpFilter struct {
    	RegexpFile  string
    	TargetFiles []string
    	ResultPath  string
    	Order       bool
    	RegexpMap   *sync.Map
    }
    ```

* `BlackTask`：每个文件抽象为一个过滤任务，其中包含了一个`sync.Map`

  ```go
  type BlackTask struct {
  	ID            int
  	BlackListName string
  	BlackMap      sync.Map // 黑名单文件扫描得出的map
  }
  ```

* `SyncWriter`：安全缓冲写

  ```go
  type SyncWriter struct {
  	m      sync.Mutex
  	Writer *bufio.Writer
  }
  ```

**封装的方法：**

* `ReadFileInLineAndDeal`：按行分块并发读取一个文件，并对行处理(注意：dealFunc函数处于多goroutine中，函数内部任何资源注意竞态并发控制)

  ```go
  // ReadFileInLineAndDeal 
  // filePath 文件路径
  // dealFunc 行处理函数
  // order 是否有序读
  func ReadFileInLineAndDeal(filePath string, dealFunc func(line string) error, order bool) error
  ```

**错误处理：**

* 采用错误`channel`的方式，一旦子协程向错误`channel`中写入错误，那么就层层返回

### 3. 问题与解决

1. 问题1: 字符串行末尾的 `\r\n`以及相同字符串但是最后多个几个空格导致匹配不上

   解决： 比较之前预处理

2. 问题2: 自造测试集测试出现问题：有些没有过滤掉，并且具有随机性（每次没过滤掉的不同）

   解决：断点排查发现`Sync.Map`的合并函数出错

3. 问题3: 缓冲writer写入结果文件时报错：`runtime error: slice bounds out of range [:4195] with capacity 4096`

   原因：共享访问问题：因为修改了代码多个协程同时用一个writer写入，其默认缓冲大小是4096，当缓冲区满了此协程要写入到静态文件时可能发生了协程切换/中断，此时另一个协程又向缓冲区中写入导致溢出。

​	   解决：安全同步写（加锁）

## 1.4 测试

### 1. 测试集生成

使用`python`生成多个黑名单、目标文件，输入参数为字符串行数量表示目标文件共有多少行（控制其大小），生成每一行随机字符串时有1/4的概率在改行后添加一些其他特殊字符组合成新的行同时加入目标文件和黑名单文件，如果概率未命中该行同时也写入对应结果文件（所以结果文件一定没有特殊字符`+-/.;) (*&^%$#@!\{\}[]?`）。（实际情况下黑名单还会有目标文件不存在的字符串行，但是在大测试文件的生成过程中为了不对目标文件之前生成的行产生影响，所以没有考虑）

过滤结果验证：

* 对于小文件可以比对总行数，用vscode自带文件逐行比对
* 对于超大文件可以先比较一下行数，然后比较其准确的字节数是否一样（如果结果乱序字节大小也是一样的）

共生成如下规格数据集：**2个黑名单文件，3个目标文件**

| amount行数 | Blacklist_0.txt | Blacklist_1.txt | target_0.txt | target_1.txt | target_2.txt |
| ---------- | --------------- | --------------- | ------------ | ------------ | ------------ |
| 1000       | 59K             | 61K             | 156K         | 155K         | 155K         |
| 10000      | 596K            | 604K            | 1.6M         | 1.5M         | 1.6M         |
| 100000     | 5.9M            | 5.9M            | 15M          | 15M          | 15M          |
| 1000000    | 59M             | 59M             | 150M         | 150M         | 150M         |
| 3000000    | 176M            | 176M            | 449M         | 449M         | 449M         |
| 5000000    | 293M            | 294M            | 748M         | 748M         | 748M         |

### 2. 代码效率评估

> 测试机器硬件环境：
>
> * Ubuntu 18 server
> * Inter i56核 16G内存

使用go自带的`pprof`工具抽样测试时间与内存，得到如下表格:

| amount行数 | 时间       | 常驻内存(infuse_space) | （乱序）时间 | （乱序）常驻内存(infuse_space) |
| ---------- | ---------- | ---------------------- | ------------ | ------------------------------ |
| 1000       | 可忽略不计 | 7.36MB                 | 可忽略不计   | 9.97MB                         |
| 10000      | 50ms       | 24.20MB                | 60ms         | 83.54MB                        |
| 100000     | 620ms      | 185.65MB               | 660ms        | 743.80MB                       |
| 1000000    | 8.66s      | 1.58GB                 | 8.25s        | 4.08GB                         |
| 3000000    | 25.85s     | 4.91GB                 | 38.03s       | 6.50GB                         |
| 5000000    | 62.07s     | 8.74GB                 | 62.48s       | 13.53GB                        |

* 虽然测量的次数很少（没有取平均），但是当数据量变大随之常驻内存的也很大
* 乱序貌似也没有多少提升

### 3. 分析与总结

以5000000行的火焰图可以看出：

按顺序扫描:

![image-20211226235125256](http://xwjpics.gumptlu.work/qinniu_uPic/image-20211226235125256.png)

乱序扫描：

![image-20211227003628021](http://xwjpics.gumptlu.work/qinniu_uPic/image-20211227003628021.png)

* 因为对目标文件、黑名单文件都按块顺序扫描，上一个块扫描结束后下一个goroutine才可以继续，所以waitgroup的等待时间占用了一部分的`processStringChunk`时间，也是较慢的原因

* 乱序因为需要大量的抢占syncWriter，所以此部分占比很大，但不需要等待块结束，所以waitgroup时间小了很多
* 大量并发goroutine使用的资源使用sync.Pool对象池可以显著减少GC的压力

总结与收获：

1. 高并发注意热门对象的创建与回收，学会测试分析（pprof真的很好用）
2. 先完成功能在想着优化
3. 一个简单的函数背后可能有很复杂的逻辑

# 二、其他方法

因为不熟悉和时间上的原因，这些方法还停留在想法并没有完全实现且验证正确性

## 2.1 shell工具

linux中的`comm`命令可以实现文本过滤: `comm -23 [target文件] [blackList文件]`

b1.txt:

```txt
11111
2222
333
```

t1.txt:

```txt
11111
222
333
45484
```

`comm -23 ./t1.txt ./b1.txt > ./r1.txt`

```txt
222
45484
```

在多个黑名单文件、目标文件的话就需要将一个目标文件遍历黑名单文件过滤，在大量数据的情况下不知道效率

> 使用时的问题：如果黑名单文件或目标文件最后一行不加换行可能会导致结果的不正确，并且最后多一个%, 可能是不同系统换行符不同的原因?

## 2.2 Redis集合操作

问题描述可以转换为: (B-A交B)，对于大量的数据并且能够提供集合操作的数据库就是redis了，但是redis是基于内存的，数据量过大可能需要提前将部分结果保存成静态文件。



